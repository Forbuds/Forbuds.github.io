---
title: "Day 023"
date: 2023-04-05

categories:
  - Blog
tags:
  - [Boostcamp, up-stage]
toc: true
toc_sticky: true
---

## 00. 데일리 스크럼  
노션에 기록  
- [x]  bert 영상  
- [x]  기본과제3  
- [ ]  강의 9강 까지  
 
## 01. 기본과제 3 수행    
    
      

**Subword-level Language Model**    
  
 학습 목표   
 - 서브워드 토큰화(Subword tokenization)  
     - 하나의 단어를 여러개의 단위로 분리했을 때 하나의 단위    
 - 서브워드 토큰화의 장점  
     -  단어 임베딩 : 매개변수 대비 임베딩 크기가 너무 커진다.    
         - 처음에는 문자 단위 토큰화, 하지만 시퀀스 너무 길어짐.    
     - <span style="background-color:#ffdce0">Out-of-Vocabulary (OoV) 문제 해결</span>    
         - 맞는 단어가 하나도 없는 상황에서도 토큰화할 수 있다.  
- 서브워드 토큰화 중 하나인 <span style="background-color:#ffdce0">Byte Pair Encoding (BPE)</span>수행  


해결 방법  
- 파이썬 표준 라이브러리인 [re-정규식(Regular expression) 연산](https://docs.python.org/ko/3/library/re.html) 이용해 수행함.  
  - re.sub 이용했음 (자세한 문법 설명은 [이 블로그](https://ihp001.tistory.com/142) 참조함)  
- dict.get 메서드를 이용해 dictionary에 없는데도 추가할 수 있도록 만들어서 문제를 해결했음.  
- 위의 문제를 defaultdict로 해결하기도 하나 봄. 더 알아볼 것.  


참고했던 자료  
- [딥 러닝을 이용한 자연어 처리 입문 - 13-01 바이트 페어 인코딩(Byte Pair Encoding, BPE)](https://wikidocs.net/22592)  
-   [Neural Machine Translation of Rare Words with Subword Units 논문 (코드 o)](https://arxiv.org/pdf/1508.07909.pdf)  


  
## 02. Peer session    

프로젝트 주제 토론  
구인구팀  세션이 있었는데, 다들 기술 스텍도 화려하고 관심 주제도 척척 고르는 걸 보면 대단한 사람들이 모였다는 게 실감났다. 너무 신기하고 나도 저만큼 발전하고 싶다는 생각이 들면서 동시에 석사까지 했으면서 아직 저렇게 할 수 없다는 사실이 부끄러웠지만, 더 공부해야지 어쩌겠어 ㅎㅎ 👍 화이팅  

  
## 03. 학습 회고   

- 정규 표현식을 많이 사용하는 것을 알고 있었지만 직접적으로 어떻게 사용하는지 의문이었는데, 이렇게 토큰화에 사용하는 것을 경험하니 정말 간편한 라이브러리였다는 것을 체감할 수 있었다.  
- 문자열 찾고 토큰화 하는 과정이 알고리즘 테스트 ( 코딩 테스트)에서 연습하던 방식으로 짜야 했어서, 기업에서 코딩 테스트를 1차 관문으로 설정한 이유에 대해 납득하였다. 알고리즘을 천재적으로 짜라는 뜻보단, 효율적이게 짜거나 내가 원하는 기능을 코드로 구현할 수 있는 능력이 있는지를 보는 것으로 납득하였다. 앞으로 알고리즘도 꾸준히 공부해야 겠다는 생각이 들었다. 지금까지는 테스트를 위해 싫어도 공부했는데, 생각보다 코딩을 함에 있어서 유용하게 쓰일 것이다, 라는 마인드로 임해야 겠다.  
- 과제에서 이렇게 쓰는 건지 모르겠지만, dictionary 자료형에서 dict.get(i,0) 구문이 생각보다 많이 쓰이는 것 같다. 예전에 이걸 몰라서 엄청 헤멧던 기억이 있는데, 그때 확실하게 알아 뒀더니 그게 지금 빛을 발하는구나 싶었다.  
- 정렬 공부를 좀 더 해야겠다.  
  
## 04. 부캠 라디오    
너무 재밌었다 ㅎㅎ 처음에는 하는 이유를 몰라서 적극적으로 참여하지 않았었는데, 굉장히 저녁 시간에 나긋나긋한 목소리로 사연 읽어주시면서 노래틀어주는 라디오 같은 세션이었다. 사연 듣는 것도 재밌었고 노래도 추억 가득한 노래들이어서 너무 좋았다.  
오늘의 베스트는 아이유의 celebrity였다. 연구실 다닐 때 이 노래에 깊이 공감하면서 눈물이 찔끔 난 적이 있었던 만큰 나한텐 노랫말이 굉장히 좋은 노래였지만 잊고 있었는데, 마음이 힘든 시점에서 다시 들으니 감회가 새로웠다.  
  
## 05. 기본과제 4 수행    
     
** Preprocessing for NMT Model**  
        

학습 목표   
 - 번역 모델을 위한 데이터셋 전처리  
-  Collating  
-  Bucketing  
