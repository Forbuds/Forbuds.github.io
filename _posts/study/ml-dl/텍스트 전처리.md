# 텍스트 전처리

✅ 인코딩 / 임베딩

✅ 토큰화의 한 방법으로 정규 표현식을 사용하기도 함

- 텍스트의 전처리
    - 풀고자 하는 문제의 용도에 맞게 텍스트를 사전에 처리하는 작업
    - 해당 데이터를 사용하고자하는 용도에 맞게 **토큰화(tokenization)** & **정제(cleaning)** & **정규화(normalization)**하는 일을 하게 됨
        - **Tokenization**
            - 의미있는 단위로  특정 기준을 잡아 주어진 corpus에서 token이라 불리는 단위로 나누는 작업.
            - 띄어쓰기 단위로 잘라내거나 / 형태소 단위로 잘라 내거
                
                ```python
                print('단어 토큰화3 :',text_to_word_sequence(
                "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."
                ))
                
                # output
                
                단어 토큰화3 : ["don't", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', "jone's", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']
                
                ```
                
        - **Cleaning**
            - 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.
                - 불필요한 단어 제거
                    - 불용어stopword 제거 (I, my, me, over, 조사, 접미사 등)
                    - 등장 빈도가 적은 단어 **Removing rare words**
                    - 짧은 단어 제거 (영어o, 한국엉 x) **Removing words with very short length :** a, I, at, in …
                    - 정규 표현식을 사용해서 제거하는 것도 가능
        - **Normalization  ([wiki](https://en.wikipedia.org/wiki/Text_normalization))**
            - 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.
            - 대, 소문자 통합
            - **Lemmatization (표제어 추출)**
                - 가령 'am', 'are', 'is'의 표제어는 'be’
            - Steming **어간 추출**
                - To prepare text for searching, it might also be [stemmed](https://en.wikipedia.org/wiki/Stemming) (e.g. converting "flew" and "flying" both into "fly")
            - [canonicalized](https://en.wikipedia.org/wiki/Canonicalization)
                - (e.g. consistently using [American or British English spelling](https://en.wikipedia.org/wiki/American_and_British_English_spelling_differences))
            - [text to speech](https://en.wikipedia.org/wiki/Speech_synthesis)만들 때 자주 사용됨
                - "$200" would be pronounced as "two hundred dollars" in English, but as "lua selau tālā" in Samoan.[[3]](https://en.wikipedia.org/wiki/Text_normalization#cite_note-3)
                - "vi" could be pronounced as "[vie](https://en.wikipedia.org/wiki/Violet_(name))," "[vee](https://en.wikipedia.org/wiki/Vi)," or "[the sixth](https://en.wikipedia.org/wiki/Roman_numerals)" depending on the surrounding words. [[4]](https://en.wikipedia.org/wiki/Text_normalization#cite_note-msdn-4)
        - cleaning & normalization 굉장히 잘 설명해 준 블로그
            
            [NLP Cleaning and Normalization (wikidocs)](https://sophia-su.tistory.com/48)
            
- **기본과제1 설명**
    
    ### 1. 파이썬 기본 코드를 이용한 영어 텍스트 토큰화 및 전처리
    
    💡 토큰화(tokenization)는 무엇인가요?
    
    토큰화는 주어진 입력 데이터를 자연어처리 모델이 인식할 수 있는 단위로 변환해주는 방법입니다.
    
    💡 단어 단위 토큰화(word tokenization)는요?
    
    단어단위 토큰화의 경우 "단어"가 자연어처리 모델이 인식하는 단위가 됩니다. "I have a meal"이라고 하는 문장을 가지고 단어 단위 토큰화를 하면 다음과 같습니다.
    
    - ['I', 'have', 'a', 'meal']
    
    영어의 경우 대부분 공백(space)을 기준으로 단어가 정의되기 때문에 `.split()`을 이용해 쉽게 단어 단위 토큰화를 구현할 수 있습니다. 특히, 영어에서 공백을 기준으로 단어를 구분한 단어 단위 토큰화는 공백 단위 토큰화 (space tokenization)이라고도 할 수 있습니다.
    
    **1-A) 토큰화기 (tokenizer) 구현**
    
    ```python
    Jhon's book isn't popular, but he loves his book.
    ['jhon', "'s", 'book', 'is', "n't", 'popular', ',', 'but', 'he', 'loves', 'his', 'book', '.']
    
    토큰화기 구현
        공백으로 토큰을 구분하되 . , ! ? 문장 부호는 별개의 토큰으로 처리되어야 합니다.
        영문에서 Apostrophe에 해당하는 ' 는 두가지 경우에 대해 처리해야합니다.
        1. not의 준말인 n't은 하나의 토큰으로 처리되어야 합니다: don't ==> do n't
        2. 다른 Apostrophe 용법은 뒤의 글자들을 붙여서 처리합니다: 's 'm 're 등등 
        그 외 다른 문장 부호는 고려하지 않으며, 작은 따옴표는 모두 Apostrophe로 처리합니다.
        모든 토큰은 소문자로 변환되어야 합나다.
    
        힌트: 정규표현식을 안다면 re 라이브러리를 사용해 보세요!
    
        예시: 'I don't like Jenifer's work.'
        ==> ['i', 'do', 'n\'t', 'like', 'jenifer', '\'s', 'work', '.']
    
        Arguments:
        sentence -- 토큰화할 영문 문장
        
        Return:
        tokens -- 토큰화된 토큰 리스트
    ```
    
    **1-B) Vocabulary 만들기**
    
    컴퓨터는 글자를 알아볼 수 없기 때문에 각 토큰을 숫자 형식의 유일한 id에 매핑해야합니다.
    
    - ['I', 'have', 'a', 'meal'] ==> [194, 123, 2, 54]
    
    이러한 매핑은 모델 학습 전에 사전 정의되어야합니다. 이때, 모델이 다를 수 있는 토큰들의 집합과 이 매핑을 Vocab라고 흔히 부릅니다.
    
    ```python
    Vocabulary 만들기
        토큰화된 문장들을 받아 각 토큰을 숫자로 매핑하는 token2id와 그 역매핑인 id2token를 만듭니다.
        자주 안나오는 단어는 과적합을 일으킬 수 있기 때문에 빈도가 적은 단어는 [UNK] 토큰으로 처리합니다.
        이는 Unknown의 준말입니다.
        토큰의 id 번호 순서는 [UNK] 토큰을 제외하고는 자유입니다.
    
        힌트: collection 라이브러리의 Counter 객체를 활용해보세요.
    
        Arguments:
        sentences -- Vocabulary를 만들기 위한 토큰화된 문장들
        min_freq -- 단일 토큰으로 처리되기 위한 최소 빈도
                    데이터셋에서 최소 빈도보다 더 적게 등장하는 토큰은 [UNK] 토큰으로 처리되어야 합니다.
    
        Return:
        id2token -- id를 받으면 해당하는 토큰을 반환하는 리스트 
        token2id -- 토큰을 받으면 해당하는 id를 반환하는 딕셔너리
    ```
    
    **1-C) 인코딩 및 디코딩**
    
    이제 문장을 받아 토큰화하고 이들을 적절한 id들로 바꾸는 인코딩 함수를 작성해 봅시다.
    
    ```python
    인코딩
        문장을 받아 토큰화하고 이들을 적절한 id들로 바꿉니다.
        토큰화 및 인덱싱은 인자로 들어온 tokenize 함수와 인자로 주어진 token2id를 활용합니다.
        Vocab에 없는 단어는 [UNK] 토큰으로 처리합니다.
    
        Arguments:
        tokenize -- 토큰화 함수: 문장을 받으면 토큰들의 리스트를 반환하는 함수
        sentence -- 토큰화할 영문 문장
        token2id -- 토큰을 받으면 해당하는 id를 반환하는 딕셔너리
        
        Return:
        token_ids -- 문장을 인코딩하여 숫자로 변환한 리스트
    
    for sid, sentence, token_ids in zip(range(1, 5), corpus, input_ids):
        print(f"======{sid}=====")
        print(f"원문: {sentence}")
        print(f"인코딩 결과: {token_ids}"),
        print(f"디코딩 결과: {decode(token_ids, id2token)}\n")
    
    ======1=====
    원문: A young man participates in a career while the subject who records it smiles.
    
    인코딩 결과: [1, 2, 3, 0, 4, 1, 0, 5, 6, 0, 7, 0, 8, 0, 9]
    디코딩 결과: a young man [UNK] in a [UNK] while the [UNK] who [UNK] it [UNK] .
    
    ======2=====
    원문: The man is scratching the back of his neck while looking for a book in a book store.
    
    인코딩 결과: [6, 3, 10, 11, 6, 12, 13, 14, 0, 5, 15, 16, 1, 17, 4, 1, 17, 18, 9]
    디코딩 결과: the man is scratching the back of his [UNK] while looking for a book in a book store .
    
    ======3=====
    원문: A person wearing goggles and a hat is sled riding.
    
    인코딩 결과: [1, 19, 20, 21, 22, 1, 23, 10, 0, 24, 9]
    디코딩 결과: a person wearing goggles and a hat is [UNK] riding .
    
    ======4=====
    원문: A girl in a pink coat and flowered goloshes sledding down a hill.
    
    인코딩 결과: [1, 25, 4, 1, 26, 27, 22, 28, 0, 0, 29, 1, 30, 9]
    디코딩 결과: a girl in a pink coat and flowered [UNK] [UNK] down a hill
    ```
    

이 전처리 과정 중, 과제에서 말하는 vacab을 만드는 과정 이 encoding이다. 

자연어 처리에서 **텍스트**를 **숫자**로 바꾸는 여러가지 기법 중,

각 단어를 고유한 정수에 맵핑(mapping)시키는 전처리 작업 : **[Integer Encoding](https://wikidocs.net/31766) ( Vacab)**

"나는 자연어 처리를 배운다”  →(토큰화) → ['나', '는', '자연어', '처리', '를', '배운다'] → (integer Encoding) → 단어 집합 : {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5} → (one-hot encoding) → "자연어” : [0, 0, 1, 0, 0, 0]

![내가 직접 만든 자연어 전처리 과정 흐름](%E1%84%90%E1%85%A6%E1%86%A8%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%20%E1%84%8C%E1%85%A5%E1%86%AB%E1%84%8E%E1%85%A5%E1%84%85%E1%85%B5%2001bded8243e14f9dac66fe31d9d3ae3b/Untitled.png)

내가 직접 만든 자연어 전처리 과정 흐름